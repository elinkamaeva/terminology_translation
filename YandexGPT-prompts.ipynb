{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f79e31a-baaa-4ac4-9632-be858c6551ad",
   "metadata": {},
   "source": [
    "## YandexGPT Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d780b71a-fd29-431c-b8bb-3b980959dbc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T03:39:34.489768Z",
     "iopub.status.busy": "2024-05-27T03:39:34.488883Z",
     "iopub.status.idle": "2024-05-27T03:39:45.670880Z",
     "shell.execute_reply": "2024-05-27T03:39:45.670081Z",
     "shell.execute_reply.started": "2024-05-27T03:39:34.489733Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sacrebleu in /home/jupyter/.local/lib/python3.10/site-packages (2.4.2)\n",
      "Requirement already satisfied: unbabel-comet in /home/jupyter/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: rouge_score in /home/jupyter/.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: portalocker in /home/jupyter/.local/lib/python3.10/site-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.22.4)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
      "Requirement already satisfied: entmax<2.0,>=1.1 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (0.23.0)\n",
      "Requirement already satisfied: jsonargparse==3.13.1 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: pandas>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.5.3)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.24.4 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (4.25.3)\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (2.2.4)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece<0.2.0,>=0.1.96 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (0.10.3)\n",
      "Requirement already satisfied: transformers<5.0,>=4.17 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (4.41.0)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (23.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2022.7.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/jupyter/.local/lib/python3.10/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.11.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->unbabel-comet) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->unbabel-comet) (16.0.6)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.4.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.6)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.8.5)\n",
      "Requirement already satisfied: setuptools in /kernel/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (65.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->unbabel-comet) (2.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Skipping jwt as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyJWT in /home/jupyter/.local/lib/python3.10/site-packages (2.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install sacrebleu unbabel-comet rouge_score\n",
    "%pip uninstall jwt\n",
    "%pip install PyJWT -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794ba42a-8570-4e39-a35d-95c508868ea8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T03:39:45.673185Z",
     "iopub.status.busy": "2024-05-27T03:39:45.672505Z",
     "iopub.status.idle": "2024-05-27T03:39:58.801172Z",
     "shell.execute_reply": "2024-05-27T03:39:58.800361Z",
     "shell.execute_reply.started": "2024-05-27T03:39:45.673146Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import jwt\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu\n",
    "from comet import download_model, load_from_checkpoint\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Constants for Yandex API\n",
    "SERVICE_ACCOUNT_ID = \"ajee5dv037e9gjovng3h\"\n",
    "KEY_ID = \"aje4k7jpuo283qj9kt9d\"\n",
    "PRIVATE_KEY = os.environ['private-key']\n",
    "CATALOGUE_ID = \"b1g29e4i9l2uqmlp2s3u\"\n",
    "\n",
    "# Function to obtain IAM token\n",
    "def get_iam_token():\n",
    "    now = int(time.time())\n",
    "    payload = {\n",
    "        'aud': 'https://iam.api.cloud.yandex.net/iam/v1/tokens',\n",
    "        'iss': SERVICE_ACCOUNT_ID,\n",
    "        'iat': now,\n",
    "        'exp': now + 360\n",
    "    }\n",
    "    encoded_token = jwt.encode(\n",
    "        payload,\n",
    "        PRIVATE_KEY,\n",
    "        algorithm='PS256',\n",
    "        headers={'kid': KEY_ID}\n",
    "    )\n",
    "    url = 'https://iam.api.cloud.yandex.net/iam/v1/tokens'\n",
    "    response = requests.post(url, headers={'Content-Type': 'application/json'}, json={'jwt': encoded_token}).json()\n",
    "    return response['iamToken']\n",
    "\n",
    "# Function to generate a translation using Yandex API\n",
    "def generate_translation(system_text, user_text):\n",
    "    token = get_iam_token()\n",
    "    url = 'https://llm.api.cloud.yandex.net/foundationModels/v1/completion'\n",
    "\n",
    "    data = {\n",
    "        'modelUri': f'gpt://{CATALOGUE_ID}/yandexgpt/latest',\n",
    "        'completionOptions': {\n",
    "            'stream': False,\n",
    "            'temperature': 0.3,\n",
    "            'maxTokens': 200\n",
    "        },\n",
    "        'messages': [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"text\": system_text\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"text\": user_text\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers={'Authorization': 'Bearer ' + token}, json=data).json()\n",
    "    translated_text = response['result']['alternatives'][0]['message']['text']\n",
    "    return translated_text\n",
    "\n",
    "def generate_translation(system_text, user_text):\n",
    "    token = get_iam_token()\n",
    "    url = 'https://llm.api.cloud.yandex.net/foundationModels/v1/completion'\n",
    "\n",
    "    data = {\n",
    "        'modelUri': f'gpt://{CATALOGUE_ID}/yandexgpt/latest',\n",
    "        'completionOptions': {\n",
    "            'stream': False,\n",
    "            'temperature': 0.3,  # Set temperature here\n",
    "            'maxTokens': 200\n",
    "        },\n",
    "        'messages': [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"text\": system_text\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"text\": user_text\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers={'Authorization': 'Bearer ' + token}, json=data).json()\n",
    "    \n",
    "    # Check if 'result' is in the response\n",
    "    if 'result' in response:\n",
    "        if 'alternatives' in response['result'] and len(response['result']['alternatives']) > 0:\n",
    "            translated_text = response['result']['alternatives'][0]['message']['text']\n",
    "            return translated_text\n",
    "        else:\n",
    "            raise ValueError(\"No alternatives found in the result.\")\n",
    "    elif 'error' in response:\n",
    "        error_message = response['error']['message']\n",
    "        if \"rate quota limit exceed\" in error_message or \"gauge quota limit exceed\" in error_message or \"ai.textGenerationCompletionSessionsCount.count gauge quota limit exceed\" in error_message:\n",
    "            raise RuntimeError(\"Rate quota limit exceeded.\")\n",
    "        else:\n",
    "            raise ValueError(f\"API Error: {error_message}\")\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected response structure: \" + str(response))\n",
    "\n",
    "# Function to generate a translation prompt\n",
    "def generate_translation_prompt(sentence, target_language):\n",
    "    return f\"Переведи специализированный текст на {target_language} язык.\", sentence\n",
    "\n",
    "# Function to generate a prompt with term context for translation\n",
    "def generate_context_prompt_translation(sentence, topic, term, translation, target_language):\n",
    "    system_text = f\"Переведи специализированный текст на {target_language} язык.\"\n",
    "    user_text = f\"Тематика: {topic}. Термин: {term}. Перевод термина: {translation}. Текст: {sentence}\"\n",
    "    return system_text, user_text\n",
    "\n",
    "# Function to generate a prompt with term context for definition\n",
    "def generate_context_prompt_definition(sentence, topic, term, definition, target_language):\n",
    "    definition = definition.split('.')[0]\n",
    "    system_text = f\"Переведи специализированный текст на {target_language} язык.\"\n",
    "    user_text = f\"Тематика: {topic}. Термин: {term}. Определение термина: {definition}. Текст: {sentence}\"\n",
    "    return system_text, user_text\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu = sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smoothie)\n",
    "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True).score(reference, hypothesis)['rougeL'].fmeasure\n",
    "    chrf = sacrebleu.corpus_chrf([hypothesis], [[reference]]).score\n",
    "    comet_data = [{\"src\": reference, \"mt\": hypothesis, \"ref\": reference}]\n",
    "    comet_score = comet_model.predict(comet_data, batch_size=1, gpus=0)['scores'][0]\n",
    "    return bleu, rouge, chrf, comet_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655dd26-9050-4e96-9d66-4dc7d72a87a3",
   "metadata": {},
   "source": [
    "### Prompt Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787c6877-c404-40c3-8cbb-e76b5f30ddf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T15:05:48.151110Z",
     "iopub.status.busy": "2024-05-24T15:05:48.149805Z",
     "iopub.status.idle": "2024-05-24T15:05:49.994977Z",
     "shell.execute_reply": "2024-05-24T15:05:49.994065Z",
     "shell.execute_reply.started": "2024-05-24T15:05:48.151041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Russian, there are two conjunctions: ЧТО and ЧТОБЫ.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_translation('Переведи специализированный текст на английский язык.', 'В русском языке есть два союза, ЧТО и ЧТОБЫ. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0b1b39-2603-4307-afa3-8ef30fe3c506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T17:48:24.060231Z",
     "iopub.status.busy": "2024-05-22T17:48:24.059046Z",
     "iopub.status.idle": "2024-05-22T17:48:35.023899Z",
     "shell.execute_reply": "2024-05-22T17:48:35.023151Z",
     "shell.execute_reply.started": "2024-05-22T17:48:24.060198Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Russian, there are two conjunctions: ЧТО and ЧТОБЫ.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_translation('Переведи специализированный текст на английский язык.', 'Тематика: лингвистика. Термин: союз. Перевод термина: conjunction. Текст: В русском языке есть два союза, ЧТО и ЧТОБЫ. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a593b6-68bc-4f3e-b581-1182dab0a224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T17:48:38.322302Z",
     "iopub.status.busy": "2024-05-22T17:48:38.321814Z",
     "iopub.status.idle": "2024-05-22T17:48:39.774224Z",
     "shell.execute_reply": "2024-05-22T17:48:39.773615Z",
     "shell.execute_reply.started": "2024-05-22T17:48:38.322266Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Russian, there are two conjunctions: ЧТО and ЧТОБЫ.\\n\\n*ЧТО* is a conjunction that connects two clauses, and it expresses a semantic relationship between these clauses.\\n*ЧТОБЫ* is also a conjunction, but it is used to express a purpose.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_translation('Переведи текст на английский язык.', 'Тематика: лингвистика. Термин: союз. Определение термина: служебная часть речи, с помощью которой связывают между собой простые предложения в составе сложного или однородные члены предложения. Не склоняется и не спрягается, и не является членом предложения. Выражает смысловые отношения между синтаксическими единицами. Текст: В русском языке есть два союза, ЧТО и ЧТОБЫ. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd3973e-c5eb-4bf0-8c24-d2ccb940c8d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T17:48:41.766730Z",
     "iopub.status.busy": "2024-05-22T17:48:41.765835Z",
     "iopub.status.idle": "2024-05-22T17:48:43.655640Z",
     "shell.execute_reply": "2024-05-22T17:48:43.654956Z",
     "shell.execute_reply.started": "2024-05-22T17:48:41.766694Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And if, due to the relative freedom of moving particles, the diffusion of gaseous bodies occurs within a period of time measured in seconds and minutes, then the same phenomenon in liquids requires hours and days, and in solids, where molecules make their way only with great difficulty, overcoming thousands of obstacles on their way, the rate of diffusion is so slow that it takes weeks and months for any noticeable result.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_translation('Переведи текст на английский язык.', 'Тематика: физика. Термин: диффузия. Определение термина: взаимное проникновение соприкасающихся веществ друг в друга вследствие теплового движения частиц. Текст: И если, благодаря сравнительной свободе движущихся частиц, диффузия газообразных тел совершается в промежуток времени, измеряемый секундами и минутами, то то же явление в жидкостях требует часов и дней, а в твердых телах, где молекулы пробираются лишь с величайшим трудом, преодолевая на своем пути тысячи препятствий, скорость диффузии так мала, что требуются для сколько-нибудь заметного результата недели и месяцы. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182a01c-8553-40b1-beaf-ea7aaf69747e",
   "metadata": {},
   "source": [
    "## Evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afbe8f43-5e65-4d7e-bf43-2a8fe030de94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T03:41:25.406924Z",
     "iopub.status.busy": "2024-05-27T03:41:25.405797Z",
     "iopub.status.idle": "2024-05-27T03:41:25.425323Z",
     "shell.execute_reply": "2024-05-27T03:41:25.424693Z",
     "shell.execute_reply.started": "2024-05-27T03:41:25.406882Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_path = 'phys_cyberleninka.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915eef60-41f6-45f1-aa3e-a0dbc605aa04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T03:41:36.403826Z",
     "iopub.status.busy": "2024-05-27T03:41:36.402897Z",
     "iopub.status.idle": "2024-05-27T03:41:36.548635Z",
     "shell.execute_reply": "2024-05-27T03:41:36.547780Z",
     "shell.execute_reply.started": "2024-05-27T03:41:36.403786Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7eff83472940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract data from the tables\n",
    "contexts_df = pd.read_sql_query(\"SELECT * FROM contexts\", conn)\n",
    "sentences_df = pd.read_sql_query(\"SELECT * FROM sentences\", conn)\n",
    "terms_df = pd.read_sql_query(\"SELECT * FROM terms\", conn)\n",
    "\n",
    "# Add a new table for translation results if it doesn't exist\n",
    "cur.execute('''\n",
    "CREATE TABLE IF NOT EXISTS gpt_results (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    sentence_id INTEGER,\n",
    "    prompt TEXT,\n",
    "    generated_translation TEXT,\n",
    "    verified_translation TEXT,\n",
    "    bleu_score REAL,\n",
    "    rouge_score REAL,\n",
    "    chrf_score REAL,\n",
    "    comet_score REAL\n",
    ")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c512d8-d2d0-432b-b633-425d49aa59bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T03:41:38.063457Z",
     "iopub.status.busy": "2024-05-27T03:41:38.062206Z",
     "iopub.status.idle": "2024-05-27T03:42:48.662775Z",
     "shell.execute_reply": "2024-05-27T03:42:48.661935Z",
     "shell.execute_reply.started": "2024-05-27T03:41:38.063409Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:26<00:00,  5.32s/it]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../tmp/xdg_cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Created a temporary directory at /tmp/tmpz91mfa5f\n",
      "Writing /tmp/tmpz91mfa5f/_remote_module_non_scriptable.py\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "# Load COMET model\n",
    "comet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(comet_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e485314-fd70-4399-988d-81137d03cebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T16:18:06.607463Z",
     "iopub.status.busy": "2024-05-26T16:18:06.606337Z",
     "iopub.status.idle": "2024-05-26T16:18:07.278472Z",
     "shell.execute_reply": "2024-05-26T16:18:07.277694Z",
     "shell.execute_reply.started": "2024-05-26T16:18:06.607405Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample up to 3 contexts per term and limit to 1000 contexts\n",
    "sampled_contexts = []\n",
    "for term_id in terms_df['id'].unique():\n",
    "    term_contexts = contexts_df[contexts_df['term_id'] == term_id]\n",
    "    sampled_contexts.extend(term_contexts.sample(min(3, len(term_contexts))).to_dict('records'))\n",
    "    if len(sampled_contexts) >= 1000:\n",
    "        break\n",
    "\n",
    "sampled_contexts = sampled_contexts[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c0d23e-694d-42a3-997e-65fdd3900317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T16:19:09.384484Z",
     "iopub.status.busy": "2024-05-26T16:19:09.383361Z",
     "iopub.status.idle": "2024-05-26T16:19:09.405191Z",
     "shell.execute_reply": "2024-05-26T16:19:09.404445Z",
     "shell.execute_reply.started": "2024-05-26T16:19:09.384445Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the sampled contexts to a file\n",
    "with open('sampled_contexts_cl_phys.json', 'w') as file:\n",
    "    json.dump(sampled_contexts, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2b209c-3687-4bf1-ba0a-cd63bd1dceff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T03:42:48.664869Z",
     "iopub.status.busy": "2024-05-27T03:42:48.664215Z",
     "iopub.status.idle": "2024-05-27T03:42:48.731690Z",
     "shell.execute_reply": "2024-05-27T03:42:48.730913Z",
     "shell.execute_reply.started": "2024-05-27T03:42:48.664833Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the sampled contexts from the file\n",
    "with open('sampled_contexts_cl_phys.json', 'r') as file:\n",
    "    sampled_contexts = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f54c0e-6dd0-46a4-828b-6f77dfa519fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('phys_cyberleninka.db')\n",
    "cur = conn.cursor()\n",
    "query = \"SELECT * FROM gpt_results ORDER BY id DESC LIMIT 1\"\n",
    "cur.execute(query)\n",
    "last_row = cur.fetchone()\n",
    "last_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db319604-c162-4d17-b2b9-7c591d31ef7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T04:08:25.613158Z",
     "iopub.status.busy": "2024-05-27T04:08:25.611988Z",
     "iopub.status.idle": "2024-05-27T04:08:25.672040Z",
     "shell.execute_reply": "2024-05-27T04:08:25.671270Z",
     "shell.execute_reply.started": "2024-05-27T04:08:25.613115Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1095, 'term_id': 191, 'sentence_id': 1469}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_contexts[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82b9da0c-6446-4f57-a406-0572d266864f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T04:08:30.576383Z",
     "iopub.status.busy": "2024-05-27T04:08:30.575219Z",
     "iopub.status.idle": "2024-05-27T04:09:42.173846Z",
     "shell.execute_reply": "2024-05-27T04:09:42.173074Z",
     "shell.execute_reply.started": "2024-05-27T04:08:30.576341Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/844 [00:00<?, ?it/s]Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "  1%|          | 6/844 [00:16<38:34,  2.76s/it]Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "  1%|          | 7/844 [00:30<1:08:54,  4.94s/it]Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "  1%|▏         | 12/844 [00:41<46:59,  3.39s/it] Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "  2%|▏         | 16/844 [01:11<1:01:43,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 113 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# topic = 'лингвистика'\n",
    "topic = 'физика'\n",
    "\n",
    "start_index = 98\n",
    "end_index = start_index + 16\n",
    "\n",
    "processed_count = 0\n",
    "# Generate prompts and save results\n",
    "for i, row in tqdm(enumerate(sampled_contexts[start_index:end_index], start=start_index), total=len(sampled_contexts) - start_index):\n",
    "    sentence_row = sentences_df[sentences_df['id'] == row['sentence_id']].iloc[0]\n",
    "    term_row = terms_df[terms_df['id'] == row['term_id']].iloc[0]\n",
    "    \n",
    "    if not term_row['excerpt_en']:\n",
    "        continue\n",
    "\n",
    "    # Generate prompts\n",
    "    translation_prompt_ru = generate_translation_prompt(sentence_row['context_ru'], 'английский')\n",
    "    translation_prompt_en = generate_translation_prompt(sentence_row['context_en'], 'русский')\n",
    "    context_prompt_translation_ru = generate_context_prompt_translation(sentence_row['context_ru'], topic, term_row['term'], term_row['translation_en'], 'английский')\n",
    "    context_prompt_translation_en = generate_context_prompt_translation(sentence_row['context_en'], topic, term_row['translation_en'], term_row['term'], 'русский')\n",
    "    context_prompt_definition_ru = generate_context_prompt_definition(sentence_row['context_ru'], topic, term_row['term'], term_row['definition_ru'], 'английский')\n",
    "    context_prompt_definition_en = generate_context_prompt_definition(sentence_row['context_en'], topic, term_row['translation_en'], term_row['excerpt_en'], 'русский')\n",
    "\n",
    "    prompts = [\n",
    "        translation_prompt_ru, \n",
    "        translation_prompt_en, \n",
    "        context_prompt_translation_ru, \n",
    "        context_prompt_translation_en,\n",
    "        context_prompt_definition_ru,\n",
    "        context_prompt_definition_en\n",
    "    ]\n",
    "    \n",
    "    for system_text, user_text in prompts:\n",
    "        try:\n",
    "            generated_translation = generate_translation(system_text, user_text).split('\\n')[0]\n",
    "            verified_translation = sentence_row['context_en'] if 'английский' in system_text else sentence_row['context_ru']\n",
    "            \n",
    "            # Compute metrics\n",
    "            bleu, rouge, chrf, comet_score = compute_metrics(verified_translation, generated_translation)\n",
    "            \n",
    "            # Save results to the database\n",
    "            conn.execute('''\n",
    "            INSERT INTO gpt_results (sentence_id, prompt, generated_translation, verified_translation, bleu_score, rouge_score, chrf_score, comet_score)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (row['sentence_id'], system_text + \" \" + user_text, generated_translation, verified_translation, bleu, rouge, chrf, comet_score))\n",
    "        except ValueError as e:\n",
    "            print(\"Error during translation:\", e)\n",
    "        except RuntimeError as e:\n",
    "            if \"Rate quota limit exceeded\" in str(e):\n",
    "                print(\"Rate quota limit exceeded. Stopping script.\")\n",
    "                conn.commit()\n",
    "                conn.close()\n",
    "                sys.exit(e)\n",
    "                \n",
    "        processed_count += 1\n",
    "        \n",
    "        # Commit every 16 sentences\n",
    "        if processed_count % 16 == 0:\n",
    "            conn.commit()\n",
    "\n",
    "# Final commit\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(f'Processed {i} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c1c12-8a07-4cac-bf4b-351f898a5c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
